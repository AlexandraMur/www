<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://aivillage.org/posts/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://aivillage.org/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dimensionality and Adversarial Examples</title>
      <link>https://aivillage.org/posts/dimensionality-and-adversarial/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://aivillage.org/posts/dimensionality-and-adversarial/</guid>
      <description>Welcome to AI Villageâ€™s series on adversarial examples. This will focus on image classification attacks as they are simpler to work with and this series is meant to explain the attacks to hackers who have an interest in data science. The underlying principles are the same for attacks against malware/spam/security classifiers, though the implementation varies. This post focuses on the core reason why adversarial examples work, the high dimensional space our data occupies.</description>
    </item>
    
  </channel>
</rss>